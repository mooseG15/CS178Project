{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we will be learning on decision trees. We try to tackle the problem of overfitting, which\n",
    "occurs with overcomplex trees. These trees can become unstable because small variations in the data\n",
    "might result in a completely different tree being generated. Luckily, we can avoid this problem\n",
    "by either setting the maximum depth of the tree or setting the minimum number of samples required \n",
    "at a leaf node. \n",
    "We proceed to learning a decision tree regressor on the data, and specifying a maximum depth of\n",
    "20. We predict on the training data and the testing data and obtain the mean squared averages\n",
    "for both data sets. The mean squared averages are displayed as follows.\n",
    "Next, we specify a variety of maximum depths for the decision trees, ranging anywhere from \n",
    "1 - 19. We want to find out how adjusting the maximum depths changes the complexities of the \n",
    "trees, and when they begin to overfit. Which maximum depth best handles the problem of overfitting?\n",
    "For each maximum depth, we learn a decision tree regressor and calculate the mean squared average\n",
    "of the training data we predicted on, and also calculate the mean squared average of the testing\n",
    "data we predicted on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training data: 0.0357911570162\n",
      "MSE for testing data: 0.712112171593\n",
      "Depth 01 --> mse train: 0.557937953351, mse validation: 0.57414551383\n",
      "Depth 02 --> mse train: 0.505146872922, mse validation: 0.519822936078\n",
      "Depth 03 --> mse train: 0.472694791396, mse validation: 0.483718589446\n",
      "Depth 04 --> mse train: 0.451999080531, mse validation: 0.465591656337\n",
      "Depth 05 --> mse train: 0.434261655119, mse validation: 0.455494004267\n",
      "Depth 06 --> mse train: 0.418033072227, mse validation: 0.446586295059\n",
      "Depth 07 --> mse train: 0.397270867629, mse validation: 0.434863566073\n",
      "Depth 08 --> mse train: 0.377611804684, mse validation: 0.438300262859\n",
      "Depth 09 --> mse train: 0.353281895038, mse validation: 0.443817028614\n",
      "Depth 10 --> mse train: 0.32512500598, mse validation: 0.467904685154\n",
      "Depth 11 --> mse train: 0.2920177674, mse validation: 0.487800548445\n",
      "Depth 12 --> mse train: 0.25566965481, mse validation: 0.525525238568\n",
      "Depth 13 --> mse train: 0.216916242863, mse validation: 0.555434048757\n",
      "Depth 14 --> mse train: 0.178732828402, mse validation: 0.586412749513\n",
      "Depth 15 --> mse train: 0.143080936773, mse validation: 0.61639887708\n",
      "Depth 16 --> mse train: 0.11355919281, mse validation: 0.638016511422\n",
      "Depth 17 --> mse train: 0.0871032692009, mse validation: 0.671957807537\n",
      "Depth 18 --> mse train: 0.0657788229549, mse validation: 0.685808352885\n",
      "Depth 19 --> mse train: 0.0492075353514, mse validation: 0.709960727505\n",
      "Depth 20 --> mse train: 0.0357897739192, mse validation: 0.712080417074\n"
     ]
    }
   ],
   "source": [
    "##########Code for the Decision Tree:##########\n",
    "#Imported Library for Decision Trees\n",
    "from sklearn import tree\n",
    "\n",
    "#Decision Tree Learner INFO... \n",
    "# Decision Tree Variables\n",
    "depth = 20 # the maxth depth of the decision tree\n",
    "nodes = 8 # the minimum number of data to split node\n",
    "\n",
    "#learn a decision tree regressor on data, specify max depth of 20\n",
    "learner = tree.DecisionTreeRegressor(max_depth=20)\n",
    "learner.fit(Xtr, Ytr)\n",
    "YhatTrain = learner.predict(Xtr)\n",
    "YhatTest = learner.predict(Xte)\n",
    "MSETrain = np.mean((Ytr - YhatTrain)**2)\n",
    "MSETest = np.mean((Yte - YhatTest)**2)\n",
    "print('{}{}'.format(\"MSE for training data: \", MSETrain))\n",
    "print('{}{}'.format(\"MSE for testing data: \", MSETest))\n",
    "\n",
    "#try different ranges of maximum depths\n",
    "for depth in range(20):\n",
    "    learner = tree.DecisionTreeRegressor(max_depth = depth+1)\n",
    "    learner.fit(Xtr, Ytr)\n",
    "    Yhat_train = learner.predict(Xtr)\n",
    "    Yhat_test = learner.predict(Xte)\n",
    "    mseTrain = np.mean((Ytr - Yhat_train)**2)\n",
    "    mseTest = np.mean((Yte - Yhat_test)**2)\n",
    "    print(\"Depth {:02d} --> mse train: {}, mse validation: {}\".format(depth+1, mseTrain, mseTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stick to specifying a maximum depth of 20, and proceed to learning on decision trees with that \n",
    "fixed depth, and we adjust the next parameter vital to creating the decision trees we want. The\n",
    "next parameter we adjust is the minimum number of samples required at a leaf node, otherwise \n",
    "known as the min_samples_leaf parameter. We learn on a range from 2^3, up to 2^12 minimum number\n",
    "of samples required at a leaf node. We predict on the training data and on the testing data, and we\n",
    "calculate their mean squared averages. The mean squared averages of the training and testing data\n",
    "are displayed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2^3 data at leaf node --> mse train: 0.159069620689, mse validation: 0.59296293253\n",
      "2^4 data at leaf node --> mse train: 0.238927974161, mse validation: 0.515825198467\n",
      "2^5 data at leaf node --> mse train: 0.303901057014, mse validation: 0.457419046287\n",
      "2^6 data at leaf node --> mse train: 0.348795257147, mse validation: 0.42866571325\n",
      "2^7 data at leaf node --> mse train: 0.376680650869, mse validation: 0.424888119888\n",
      "2^8 data at leaf node --> mse train: 0.398765237031, mse validation: 0.429117852099\n",
      "2^9 data at leaf node --> mse train: 0.414565160586, mse validation: 0.438279933896\n",
      "2^10 data at leaf node --> mse train: 0.431358990435, mse validation: 0.450114946475\n",
      "2^11 data at leaf node --> mse train: 0.452946540527, mse validation: 0.46886867678\n",
      "2^12 data at leaf node --> mse train: 0.47046962598, mse validation: 0.483359063336\n"
     ]
    }
   ],
   "source": [
    "for nodes in range(3, 13):\n",
    "    learner = tree.DecisionTreeRegressor(max_depth = 20, min_samples_leaf = 2**nodes)\n",
    "    learner.fit(Xtr, Ytr)\n",
    "    Yhat_train = learner.predict(Xtr)\n",
    "    Yhat_test = learner.predict(Xte)\n",
    "    mseTrain = np.mean((Ytr - Yhat_train)**2)\n",
    "    mseTest = np.mean((Yte - Yhat_test)**2)\n",
    "    print(\"2^{} data at leaf node --> mse train: {}, mse validation: {}\".format(nodes, mseTrain, mseTest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to choose 2^8 minimum number of samples required at each leaf node, based on the results\n",
    "of the mean squared averages from the training and testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to storing our individual learners in an ensemble. We set the size of the ensemble\n",
    "to 20 because we will be storing 20 different learners within the ensemble. We will store the KNN and \n",
    "the decision tree learners in the ensemble. Unfortunately, we will not be able to store the Support\n",
    "Vector learners in this ensemble. \n",
    "We create 10 different decision tree learners. Using a maximum depth of 20 and 8 minimum samples required at each\n",
    "leaf node as our favorite parameters, we add each of the learners to the ensemble of different learners.\n",
    "Finally, we proceed to storing our individual learners in an ensemble. We set the size of the ensemble\n",
    "to 20 because we will be storing 20 different learners within the ensemble. We will store the KNN and \n",
    "the decision tree learners in the ensemble. Unfortunately, we will not be able to store the Support\n",
    "Vector learners in this ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Code to store each learner in the ensemble:##########\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "ensemble = [None]*10 #for now store 10 learners in ensemble before combining all learners \n",
    "# Create learners and add to dtree ensemble\n",
    "#dtLearners = BaggingRegressor(tree.DecisionTreeRegressor(max_depth = 20, min_samples_leaf = 2**8))\n",
    "M = Xtr.shape[0]\n",
    "Me = Xte.shape[0]\n",
    "YtrHat = np.zeros((M,10))\n",
    "YteHat = np.zeros((Me,10))\n",
    "for l in range(10): #add 10 decision tree learners to the ensemble\n",
    "    Xi, Yi = ml.bootstrapData(Xtr, Ytr, M)\n",
    "    ensemble[l] = BaggingRegressor(tree.DecisionTreeRegressor(max_depth=20,min_samples_leaf = 2**8))\n",
    "    ensemble[l].fit(Xi, Yi)\n",
    "    Ythat = ensemble[l].predict(Xtr)\n",
    "    Yehat = ensemble[l].predict(Xte)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
