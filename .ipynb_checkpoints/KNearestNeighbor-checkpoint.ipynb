{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodrigo Hernandez  \n",
    "Muzamil Syed  \n",
    "Mayra Gamboa  \n",
    "CS178  \n",
    "<h3 align='center'>Group Project</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<i><h2 align='center'>Introduction to Our Technique</h2></i>  \n",
    "The approach we will be taking will have to do with going into detail with Ensembles. \n",
    "We will each choose our own learning techniques and individually train on the provided data set.\n",
    "We will use multiple instances of each of our learners to store the ensemble???\n",
    "This will be done using existing packages provided by sklearn library. We will use the methods \n",
    "provided by the library and explore additional techniques to supplement these methods to vary the\n",
    "complexities of the models that result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Libraries Used Throughout The Code:##########\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import mltools as ml\n",
    "import mltools.dtree as dtree\n",
    "import mltools.logistic2 as lcs2\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000L, 91L)\n"
     ]
    }
   ],
   "source": [
    "##########Imported Data:##########\n",
    "'''\n",
    "We will use the provided Class Kaggle Data in our class Kaggle Competition: CS178 Project 2016 \n",
    "\n",
    "https://inclass.kaggle.com/c/cs178-project-2016\n",
    "'''  \n",
    "\n",
    "# Get Kaggle training data\n",
    "X = np.genfromtxt(\"data/kaggle.X1.train.txt\",delimiter=\",\")\n",
    "Y = np.genfromtxt(\"data/kaggle.Y.train.txt\",delimiter=\",\")\n",
    "\n",
    "# also load features of the test data (to be predicted)\n",
    "Xe1 = np.genfromtxt(\"data/kaggle.X1.test.txt\",delimiter=\",\")\n",
    "\n",
    "perSplit = 0.8 # Percent at which to split the training data\n",
    "               # (e.g 0.8 = 80/20 split)\n",
    "\n",
    "Xtr,Xte,Ytr,Yte = ml.splitData(X,Y,0.8)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Initialization of the Ensemble:##########\n",
    "\n",
    "'''\n",
    "We will start with an Ensemble of size 25...\n",
    "''' \n",
    "\n",
    "# Ensemble Variables\n",
    "size = 25  # the amount of learners in the ensemble\n",
    "features = 55 # the number of features to select from when bagging\n",
    "\n",
    "# Create the ensemble\n",
    "ensemble = [None] * size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for K =  1 Neighbors     Error(Training) =  0.0000     Error(Validation) =  0.0000\n",
      "MSE for K =  2 Neighbors     Error(Training) =  0.1863     Error(Validation) =  0.2122\n",
      "MSE for K =  3 Neighbors     Error(Training) =  0.2555     Error(Validation) =  0.2805\n",
      "MSE for K =  4 Neighbors     Error(Training) =  0.2932     Error(Validation) =  0.3197\n",
      "MSE for K =  5 Neighbors     Error(Training) =  0.3175     Error(Validation) =  0.3458\n",
      "MSE for K =  6 Neighbors     Error(Training) =  0.3352     Error(Validation) =  0.3676\n",
      "MSE for K =  7 Neighbors     Error(Training) =  0.3486     Error(Validation) =  0.3826\n",
      "MSE for K =  8 Neighbors     Error(Training) =  0.3578     Error(Validation) =  0.3965\n",
      "MSE for K =  9 Neighbors     Error(Training) =  0.3655     Error(Validation) =  0.4033\n",
      "MSE for K = 10 Neighbors     Error(Training) =  0.3719     Error(Validation) =  0.4087\n",
      "MSE for K = 11 Neighbors     Error(Training) =  0.3771     Error(Validation) =  0.4145\n",
      "MSE for K = 12 Neighbors     Error(Training) =  0.3812     Error(Validation) =  0.4190\n",
      "MSE for K = 13 Neighbors     Error(Training) =  0.3855     Error(Validation) =  0.4221\n",
      "MSE for K = 14 Neighbors     Error(Training) =  0.3895     Error(Validation) =  0.4259\n",
      "MSE for K = 15 Neighbors     Error(Training) =  0.3928     Error(Validation) =  0.4289\n",
      "MSE for K = 16 Neighbors     Error(Training) =  0.3964     Error(Validation) =  0.4311\n",
      "MSE for K = 17 Neighbors     Error(Training) =  0.3993     Error(Validation) =  0.4340\n",
      "MSE for K = 18 Neighbors     Error(Training) =  0.4020     Error(Validation) =  0.4370\n",
      "MSE for K = 19 Neighbors     Error(Training) =  0.4041     Error(Validation) =  0.4393\n",
      "MSE for K = 20 Neighbors     Error(Training) =  0.4062     Error(Validation) =  0.4417\n"
     ]
    }
   ],
   "source": [
    "##########Code for the KNN Learner:##########\n",
    "from sklearn.neighbors import KNeighborsRegressor  #Imported Library for KNN Regressor \n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "#print Xtr.shape, Ytr.shape\n",
    "\n",
    "#Create a list to store the top KnnRegressor Performers \n",
    "topKnnPerformers = [None]*10\n",
    "\n",
    "#Predict and print the MSE for training and test data on knnRegressors from 1 to 20 Nearest Neighbors\n",
    "for K in range(1, 21):\n",
    "    knnRegressor = KNeighborsRegressor(n_neighbors = K)\n",
    "    yhatTr = knnRegressor.fit(Xtr, Ytr).predict(Xtr) \n",
    "    yhatVa = knnRegressor.fit(Xte, Yte).predict(Xte) \n",
    "    print \"MSE for K = {:>2} Neighbors     Error(Training) = {:>7.4f}     Error(Validation) = {:>7.4f}\".format(\n",
    "            K, np.mean((Ytr - yhatTr)**2), np.mean((Yte - yhatVa)**2))\n",
    "    \n",
    "    #Since the first 10 regressors  \n",
    "    if (K <= 10):\n",
    "        topKnnPerformers[K-1] = knnRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
      "          weights='uniform'), KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "          weights='uniform')]\n"
     ]
    }
   ],
   "source": [
    "print(topKnnPerformers) #checking to see if the first 10 knn neighbors were stored in the list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the KNeighborsRegressor from the sklearn library a knnregressor learner was constructed with neigbors ranging from 1 to 20. It was expected, as the commentary mentioned on the project writeup that the KNN learners would take some time to train and predict. The process was ended up being fast (30 minutes - 1 hour) and the resulting MSE was very low giving us good results for the FULL data set. As the results are seen above, the error on the training data was  < ~.41 and < ~.44 for the test data. Because the model starts underfitting for larger values of K we decided to move forward with neighbors less than K = 10, meaning taking the top 10 performers to store into the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-131a9d428a1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mknnEnsemble\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaggingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopKnnPerformers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0myhatTr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknnEnsemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0myhatVa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknnEnsemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXte\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\muzzimsyed21\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\bagging.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 verbose=self.verbose)\n\u001b[1;32m--> 337\u001b[1;33m             for i in range(n_jobs))\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\muzzimsyed21\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\muzzimsyed21\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\muzzimsyed21\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\muzzimsyed21\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\muzzimsyed21\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\muzzimsyed21\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\bagging.pyc\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, verbose)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "topEnsemblePerformers = [None]*10\n",
    "\n",
    "for i, regressor in enumerate(topKnnPerformers):\n",
    "    \n",
    "    knnEnsemble = BaggingRegressor(base_estimator = topKnnPerformers[i], n_estimators = 10)\n",
    "    yhatTr = knnEnsemble.fit(Xtr, Ytr).predict(Xtr) \n",
    "    yhatVa = knnEnsemble.fit(Xte, Yte).predict(Xte)\n",
    "    \n",
    "    print \"MSE for Ensemble with a K = {:>2} Neighbors Regressor     Error(Training) = {:>7.4f}     Error(Validation) = {:>7.4f}\".format(\n",
    "            regressor.n_neighbors, np.mean((Ytr - yhatTr)**2), np.mean((Yte - yhatVa)**2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for Ensemble with a K =  1 Neighbors Regressor     Error(Training) =  0.0955     Error(Validation) =  0.1055 <br>\n",
    "MSE for Ensemble with a K =  2 Neighbors Regressor     Error(Training) =  0.1729     Error(Validation) =  0.1936 <br>\n",
    "MSE for Ensemble with a K =  3 Neighbors Regressor     Error(Training) =  0.2359     Error(Validation) =  0.2596 <br> \n",
    "\n",
    "Playing around with the BaggingRegressor library, there was no way to store the 10 topKnnPerformers into one Ensemble so we tested storing 10 of each topKnnPerformers and train/predicted to see the results. We found that since it was taking too long to output the MSE and this approach just did not make sense, so we did not move forward with it. However, it is worth noting the drop in MSE for the 3 outputs that did end up printing as shown above. Afterwards, reading through library documentation we moved forward with another approach below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Ensemble with top 10 KnnRegressors     Error(Training) =  0.0719     Error(Validation) =  0.0773\n"
     ]
    }
   ],
   "source": [
    "knnEnsemble = BaggingRegressor(n_estimators = len(topKnnPerformers))\n",
    "knnEnsemble.estimators_ = topKnnPerformers\n",
    "yhatTr = knnEnsemble.fit(Xtr, Ytr).predict(Xtr)\n",
    "yhatVa = knnEnsemble.fit(Xte, Yte).predict(Xte)\n",
    "\n",
    "print \"MSE for Ensemble with top 10 KnnRegressors     Error(Training) = {:>7.4f}     Error(Validation) = {:>7.4f}\".format(\n",
    "        np.mean((Ytr - yhatTr)**2), np.mean((Yte - yhatVa)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since BaggingRegressors library had an attribute to store subset of estimators, we decided to take our list of topPerformers trained previously and store this list to that attribute. This allowed us to train and predicted on the entire list of learners rather than having to indivudally store them in a BaggingRegressor with 10 copies of each knnRegressor. After training and predicting on this entire list of already well performing learners, the result was an even lower MSE!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Code to store each learner in the ensemble:##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Code to output the predictions and evaluate them on kaggle:##########\n",
    "\n",
    "# Test correctness of ensemble through MSE\n",
    "mTest = Xte.shape[0] # Acquire the shape of the test data\n",
    "Yhat = np.zeros((mTest,num))\n",
    "MSE = 0\n",
    "\n",
    "for i in range(size):\n",
    "    Yhat[:,i] = ensemble[i].predict(Xte).reshape(mTest)\n",
    "    \n",
    "    Yhat = np.mean(Yhat,axis=1)\n",
    "    \n",
    "    MSE = np.mean((Yte - Yhat.reshape(Yte.shape))**2,axis=0)\n",
    "    \n",
    "print(MSE)\n",
    "\n",
    "'''\n",
    "Note:\n",
    "-Should not try to upload every possible model with every possible parameter setting \n",
    "-Use validation data, or cross-validation to assess which models are worth uploading, and just use the uploads\n",
    "to verify performance. \n",
    "'''\n",
    "#Ye = learner.predict( Xeval ); # make predictions\n",
    "# Note: be sure Ye is a flat vector, shape (m,)\n",
    "# otherwise, reshape it using e.g.\n",
    "# Ye = Ye.ravel()\n",
    "# or change the indexing in the code below:\n",
    "fh = open('predictions.csv','w') # open file for upload\n",
    "fh.write('ID,Prediction\\n') # output header line\n",
    "for i,yi in enumerate(Ye):\n",
    "fh.write('{},{}\\n'.format(i+1,yi)) # output each prediction\n",
    "fh.close() # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<i><h2 align='center'>Conclusion</h2></i>  \n",
    "Here we can probably summarize our results and the learners we were responsible for.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
