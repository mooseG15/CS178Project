{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodrigo Hernandez  \n",
    "Muzamil Syed  \n",
    "Mayra Gamboa  \n",
    "CS178  \n",
    "<h3 align='center'>Group Project</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<i><h2 align='center'>Introduction to Our Technique</h2></i>  \n",
    "The approach we will be taking will have to do with going into detail with Ensembles. \n",
    "We will each choose our own learning techniques and individually train on the provided data set.\n",
    "We will use multiple instances of each of our learners to store the ensemble???\n",
    "This will be done using existing packages provided by sklearn library. We will use the methods \n",
    "provided by the library and explore additional techniques to supplement these methods to vary the\n",
    "complexities of the models that result. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions\n",
    "Are we bagging (bootstrap) where we use part of the data?  \n",
    "To prevent overfitting we are using enhancement techniques for each our learners? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Report Format: (Maybe me we could have some kind of title page added to final pdf with our names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Libraries Used Throughout The Code:##########\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import mltools as ml\n",
    "import mltools.dtree as dtree\n",
    "import mltools.logistic2 as lcs2\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "#Imported Library for Neural Network \n",
    "#Imported Library for Third Learner \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 91)\n"
     ]
    }
   ],
   "source": [
    "##########Imported Data:##########\n",
    "import numpy as np\n",
    "'''\n",
    "We will use the provided Class Kaggle Data in our class Kaggle Competition: CS178 Project 2016 \n",
    "\n",
    "https://inclass.kaggle.com/c/cs178-project-2016\n",
    "'''  \n",
    "\n",
    "# Get Kaggle training data\n",
    "X = np.genfromtxt(\"data/kaggle.X1.train.txt\",delimiter=\",\")\n",
    "Y = np.genfromtxt(\"data/kaggle.Y.train.txt\",delimiter=\",\")\n",
    "\n",
    "# also load features of the test data (to be predicted)\n",
    "Xe1 = np.genfromtxt(\"data/kaggle.X1.test.txt\",delimiter=\",\")\n",
    "\n",
    "perSplit = 0.8 # Percent at which to split the training data\n",
    "               # (e.g 0.8 = 80/20 split)\n",
    "\n",
    "Xtr,Xte,Ytr,Yte = ml.splitData(X,Y,0.8)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Initialization of the Ensemble:##########\n",
    "\n",
    "'''\n",
    "We will start with an Ensemble of size 25...\n",
    "''' \n",
    "\n",
    "# Ensemble Variables\n",
    "size = 25  # the amount of learners in the ensemble\n",
    "features = 55 # the number of features to select from when bagging\n",
    "\n",
    "# Create the ensemble\n",
    "ensemble = [None] * size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Code for the KNN Learner:##########\n",
    "from sklearn.neighbors import KNeighborsRegressor  #Imported Library for KNN Regressor \n",
    "\n",
    "'''\n",
    "KNN Learner INFO... \n",
    "-describe problem you use chose and methods to address it\n",
    "-how did you train the models?\n",
    "-how you selected any parameters each model/method requires \n",
    "-how they performed on test data\n",
    "-consider table of performance of different approaches or plots of perofrmance used to perform model selection\n",
    "''' \n",
    "\n",
    "'''\n",
    "Key Points:\n",
    "-Explore some aspect of prediction that we have not already done in depth \n",
    "-Identify a paper that proposes a method you think could be helpful \n",
    "-Use stacking/information from your leaderboard performance to try and improve your prediction quality \n",
    "-To explore approach: explore method from class fully enough to understand how changes might affect its performance,\n",
    "verify your findings make sense, and then use your findings to optimize performance\n",
    "-In Your Report: describe why you decided to explore this aspect, what you expected to find, and how your findings\n",
    "matched/ didnt match your expectations \n",
    "-Beware of the positive/negative aspects of the learners we discussed ie Nearest Neighbor methods can be powerful but can\n",
    "also be slow for large data sets...perhaps you can reduce the data in some way without sacrificing performance (bootstrap\n",
    "aggregation)\n",
    "-Linear methods may be fast but do not provide enough model complexity to provide a good fit so you may been to try and \n",
    "generate better features \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Code for the Support Vector Machines:##########\n",
    "\n",
    "'''\n",
    "Support Vector Machine INFO... \n",
    "'''\n",
    "Xi,Yi = ml.utils.bootstrapData(Xtr,Ytr)\n",
    "clf = svm.SVR(kernel='poly',degree=2,cache_size=7000)\n",
    "#print(Xi.shape)\n",
    "\n",
    "A = Xi[0:30,0:55]\n",
    "B = Yi[0:30]\n",
    "print(Yi.shape)\n",
    "# SVMensemble = sklearn.ensemble.BaggingRegressor(clf,7,max_samples=25,\n",
    "#  max_features=features)\n",
    "\n",
    "clf.fit(A,B)\n",
    "print(clf.predict(Xte[0:2,0:55]))\n",
    "# SVMensemble.fit(Xtr,Ytr)\n",
    "# SVMensemble.predict(Xte[0,:])\n",
    "\n",
    "#Imported Library for Neural Network  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training data: 0.0357911227183\n",
      "MSE for testing data: 0.726991388748\n",
      "\n",
      "\n",
      "Depth 01 --> mse train: 0.557937953351, mse validation: 0.57414551383\n",
      "Depth 02 --> mse train: 0.505146872922, mse validation: 0.519822936078\n",
      "Depth 03 --> mse train: 0.472694791396, mse validation: 0.483718589446\n",
      "Depth 04 --> mse train: 0.451999080531, mse validation: 0.465591656337\n",
      "Depth 05 --> mse train: 0.434261655119, mse validation: 0.455494004267\n",
      "Depth 06 --> mse train: 0.418033072227, mse validation: 0.446586295059\n",
      "Depth 07 --> mse train: 0.397270867629, mse validation: 0.43521071237\n",
      "Depth 08 --> mse train: 0.377611804684, mse validation: 0.440208874745\n",
      "Depth 09 --> mse train: 0.353281895038, mse validation: 0.446513148684\n",
      "Depth 10 --> mse train: 0.32512500598, mse validation: 0.464664335836\n",
      "Depth 11 --> mse train: 0.2920177674, mse validation: 0.484219951949\n",
      "Depth 12 --> mse train: 0.25566965481, mse validation: 0.514424668305\n",
      "Depth 13 --> mse train: 0.216916698559, mse validation: 0.55258222697\n",
      "Depth 14 --> mse train: 0.178733837642, mse validation: 0.580236633767\n",
      "Depth 15 --> mse train: 0.143084770324, mse validation: 0.621130636606\n",
      "Depth 16 --> mse train: 0.113559591856, mse validation: 0.645537962467\n",
      "Depth 17 --> mse train: 0.0871034107474, mse validation: 0.664065477858\n",
      "Depth 18 --> mse train: 0.0657787643252, mse validation: 0.687218791811\n",
      "Depth 19 --> mse train: 0.0492076079306, mse validation: 0.698372279597\n",
      "\n",
      "\n",
      "2^3 data at leaf node --> mse train: 0.159068293718, mse validation: 0.593354378406\n",
      "2^4 data at leaf node --> mse train: 0.238927974161, mse validation: 0.515669493421\n",
      "2^5 data at leaf node --> mse train: 0.303901057014, mse validation: 0.457412998062\n",
      "2^6 data at leaf node --> mse train: 0.348795257147, mse validation: 0.428665778034\n",
      "2^7 data at leaf node --> mse train: 0.376680650869, mse validation: 0.424888196308\n",
      "2^8 data at leaf node --> mse train: 0.398765237031, mse validation: 0.42911833535\n",
      "2^9 data at leaf node --> mse train: 0.414565160586, mse validation: 0.438279933896\n",
      "2^10 data at leaf node --> mse train: 0.431358990435, mse validation: 0.450114946475\n",
      "2^11 data at leaf node --> mse train: 0.452946540527, mse validation: 0.46886867678\n",
      "2^12 data at leaf node --> mse train: 0.47046962598, mse validation: 0.483359063336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWe decide to choose 2^8 minimum number of samples required at each leaf node, based on the results\\nof the mean squared averages from the training and testing data. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########Code for the Decision Tree:##########\n",
    "#Imported Library for Decision Trees\n",
    "'''\n",
    "Decision Tree Learner INFO... \n",
    "'''\n",
    "\n",
    "'''\n",
    "Here we will be learning on decision trees. We try to tackle the problem of overfitting, which\n",
    "occurs with overcomplex trees. These trees can become unstable because small variations in the data\n",
    "might result in a completely different tree being generated. Luckily, we can avoid this problem\n",
    "by either setting the maximum depth of the tree or setting the minimum number of samples required \n",
    "at a leaf node. \n",
    "'''\n",
    "\n",
    "# Decision Tree Variables\n",
    "depth = 20 # the maxth depth of the decision tree\n",
    "nodes = 8 # the minimum number of data to split node\n",
    "\n",
    "'''\n",
    "We proceed to learning a decision tree regressor on the data, and specifying a maximum depth of\n",
    "20. We predict on the training data and the testing data and obtain the mean squared averages\n",
    "for both data sets. The mean squared averages are as follows:\n",
    "'''\n",
    "#learn a decision tree regressor on data, specify max depth of 20\n",
    "learner = tree.DecisionTreeRegressor(max_depth=20)\n",
    "learner.fit(Xtr, Ytr)\n",
    "YhatTrain = learner.predict(Xtr)\n",
    "YhatTest = learner.predict(Xte)\n",
    "MSETrain = np.mean((Ytr - YhatTrain)**2)\n",
    "MSETest = np.mean((Yte - YhatTest)**2)\n",
    "print('{}{}'.format(\"MSE for training data: \", MSETrain))\n",
    "print('{}{}'.format(\"MSE for testing data: \", MSETest))\n",
    "print('\\n')\n",
    "\n",
    "'''\n",
    "Now, we will specify a variety of maximum depths for the decision trees, ranging anywhere from \n",
    "1 - 19. We want to find out how adjusting the maximum depths changes the complexities of the \n",
    "trees, and when they begin to overfit. Which maximum depth best handles the problem of overfitting?\n",
    "For each maximum depth, we learn a decision tree regressor and calculate the mean squared average\n",
    "of the training data we predicted on, and also calculate the mean squared average of the testing\n",
    "data we predicted on. \n",
    "'''\n",
    "for depth in range(19):\n",
    "    learner = tree.DecisionTreeRegressor(max_depth = depth+1)\n",
    "    learner.fit(Xtr, Ytr)\n",
    "    Yhat_train = learner.predict(Xtr)\n",
    "    Yhat_test = learner.predict(Xte)\n",
    "    mseTrain = np.mean((Ytr - Yhat_train)**2)\n",
    "    mseTest = np.mean((Yte - Yhat_test)**2)\n",
    "    print(\"Depth {:02d} --> mse train: {}, mse validation: {}\".format(depth+1, mseTrain, mseTest))\n",
    "print('\\n')\n",
    "\n",
    "'''\n",
    "We stick to specifying a maximum depth of 20, and proceed to learning on decision trees with that \n",
    "fixed depth, and we adjust the next parameter vital to creating the decision trees we want. The\n",
    "next parameter we adjust is the minimum number of samples required at a leaf node, otherwise \n",
    "known as the min_samples_leaf parameter. We learn on a range from 2^3, up to 2^12 minimum number\n",
    "of samples required at a leaf node. We predict on the training data and on the testing data, and we\n",
    "calculate their mean squared averages. The mean squared averages of the training and testing data\n",
    "are as follows:\n",
    "'''\n",
    "for nodes in range(3, 13):\n",
    "    learner = tree.DecisionTreeRegressor(max_depth = 20, min_samples_leaf = 2**nodes)\n",
    "    learner.fit(Xtr, Ytr)\n",
    "    Yhat_train = learner.predict(Xtr)\n",
    "    Yhat_test = learner.predict(Xte)\n",
    "    mseTrain = np.mean((Ytr - Yhat_train)**2)\n",
    "    mseTest = np.mean((Yte - Yhat_test)**2)\n",
    "    print(\"2^{} data at leaf node --> mse train: {}, mse validation: {}\".format(nodes, mseTrain, mseTest))\n",
    "\n",
    "'''\n",
    "We decide to choose 2^8 minimum number of samples required at each leaf node, based on the results\n",
    "of the mean squared averages from the training and testing data. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Code to output the predictions and evaluate them on kaggle:##########\n",
    "\n",
    "# Test correctness of ensemble through MSE\n",
    "mTest = Xte.shape[0] # Acquire the shape of the test data\n",
    "Yhat = np.zeros((mTest,num))\n",
    "MSE = 0\n",
    "\n",
    "for i in range(size):\n",
    "    Yhat[:,i] = ensemble[i].predict(Xte).reshape(mTest)\n",
    "    \n",
    "    Yhat = np.mean(Yhat,axis=1)\n",
    "    \n",
    "    MSE = np.mean((Yte - Yhat.reshape(Yte.shape))**2,axis=0)\n",
    "    \n",
    "print(MSE)\n",
    "\n",
    "'''\n",
    "Note:\n",
    "-Should not try to upload every possible model with every possible parameter setting \n",
    "-Use validation data, or cross-validation to assess which models are worth uploading, and just use the uploads\n",
    "to verify performance. \n",
    "'''\n",
    "#Ye = learner.predict( Xeval ); # make predictions\n",
    "# Note: be sure Ye is a flat vector, shape (m,)\n",
    "# otherwise, reshape it using e.g.\n",
    "# Ye = Ye.ravel()\n",
    "# or change the indexing in the code below:\n",
    "fh = open('predictions.csv','w') # open file for upload\n",
    "fh.write('ID,Prediction\\n') # output header line\n",
    "for i,yi in enumerate(Ye):\n",
    "fh.write('{},{}\\n'.format(i+1,yi)) # output each prediction\n",
    "fh.close() # close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Code to store each learner in the ensemble:##########\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "'''\n",
    "How should we store the learners we come up with? Since right now we have an ensemble of size 25 we could maybe \n",
    "create a list for each of our learners and bring them here then index each list of learners, train it and then store \n",
    "it into the ensemble before making all the predictions. What do you guys think?  \n",
    "'''\n",
    "\n",
    "'''\n",
    "Now we proceed to storing our individual learners in an ensemble. We set the size of the ensemble\n",
    "to 25 because we will be storing 25 different learners within the ensemble. We will store the KNN and \n",
    "the decision tree learners in the ensemble. Unfortunately, we will not be able to store the Support\n",
    "Vector learners in this ensemble.\n",
    "'''\n",
    "\n",
    "ensemble = [None]*25\n",
    "# Create learners and add to ensemble\n",
    "#dtLearners = BaggingRegressor(tree.DecisionTreeRegressor(max_depth = 20, min_samples_leaf = 2**nodes))\n",
    "\n",
    "'''\n",
    "We proceed to creating 8 different decision tree learners, we might create more....\n",
    "Using a maximum depth of 20 and 8 minimum samples required at each leaf node as our favorite \n",
    "parameters, we add each of the learners to the ensemble of different learners. \n",
    "'''\n",
    "M = Xtr.shape[0]\n",
    "Me = Xte.shape[0]\n",
    "YtrHat = np.zeros((M,8))\n",
    "YteHat = np.zeros((Me,8))\n",
    "for l in range(8): #add 8 decision tree learners to the ensemble\n",
    "    Xi, Yi = ml.bootstrapData(Xtr, Ytr, M)\n",
    "    ensemble[l] = BaggingRegressor(tree.DecisionTreeRegressor(max_depth=20,min_samples_leaf = 2**8))\n",
    "    ensemble[l].fit(Xi, Yi)\n",
    "    Ythat = ensemble[l].predict(Xtr)\n",
    "    Yehat = ensemble[l].predict(Xte)\n",
    "    \n",
    "'''\n",
    "For each learner in the ensemble, we predict on the training data and the testing data.\n",
    "'''\n",
    "    \n",
    "\n",
    "#for i in range(size):\n",
    "    \n",
    "    #for learners in each list \n",
    "    \n",
    "        #get KNN learner \n",
    "        #train KNN learner \n",
    "        #ensemble[i] = KNN Learner \n",
    "        \n",
    "        #get NN learner \n",
    "        #train NN learner \n",
    "        #ensemble[i] = NN Learner \n",
    "\n",
    "        #get DT learner \n",
    "        #train DT learner \n",
    "        #ensemble[i] = DT Learner \n",
    "        \n",
    "    #dt = dtree.treeRegress()\n",
    "    #Xi,Yi = ml.utils.bootstrapData(Xtr,Ytr)\n",
    "    #dt.train(Xi,Yi,maxdepth=depth,nFeatures=features,minParent=nodes)\n",
    "    \n",
    "    #ensemble[i] = dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<i><h2 align='center'>Conclusion</h2></i>  \n",
    "Here we can probably summarize our results and the learners we were responsible for.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
