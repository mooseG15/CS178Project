{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodrigo Hernandez  \n",
    "Muzamil Syed  \n",
    "Mayra Gamboa  \n",
    "CS178  \n",
    "<h3 align='center'>Group Project</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<i><h2 align='center'>Introduction to Our Technique</h2></i>  \n",
    "The approach we will be taking will have to do with going into detail with Ensembles. Instead of having an ensemble of only one type of learner, we decided to have an ensemble with three different types of learners. These learners will be decision trees, K-Nearest-Neighbours, and support vector machines.  \n",
    "\n",
    "We will each choose our own type of learner and perform our own experiments to try and maximize the accuracy of our learner's predictions. After we have chosen our optimal learners, we will combine them into an ensemble of size 24 (8 learners of each type) and predict the test data using this new ensemble.  \n",
    "\n",
    "We will use existing packages provided by sklearn library. We will use the methods provided by the library and explore additional techniques to supplement these methods to vary the complexities of the models that result."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions\n",
    "Are we bagging (bootstrap) where we use part of the data?  \n",
    "To prevent overfitting we are using enhancement techniques for each our learners? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Report Format: (Maybe me we could have some kind of title page added to final pdf with our names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Libraries Used Throughout The Code:##########\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import mltools as ml\n",
    "import mltools.dtree as dtree\n",
    "import mltools.logistic2 as lcs2\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "#Imported Library for Neural Network \n",
    "#Imported Library for Third Learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 91)\n"
     ]
    }
   ],
   "source": [
    "##########Imported Data:##########\n",
    "\n",
    "'''\n",
    "We will use the provided Class Kaggle Data in our class Kaggle Competition: CS178 Project 2016 \n",
    "\n",
    "https://inclass.kaggle.com/c/cs178-project-2016\n",
    "'''  \n",
    "\n",
    "# Get Kaggle training data\n",
    "X = np.genfromtxt(\"data/kaggle.X1.train.txt\",delimiter=\",\")\n",
    "Y = np.genfromtxt(\"data/kaggle.Y.train.txt\",delimiter=\",\")\n",
    "\n",
    "# also load features of the test data (to be predicted)\n",
    "Xe1 = np.genfromtxt(\"data/kaggle.X1.test.txt\",delimiter=\",\")\n",
    "\n",
    "perSplit = 0.8 # Percent at which to split the training data\n",
    "               # (e.g 0.8 = 80/20 split)\n",
    "\n",
    "Xtr,Xte,Ytr,Yte = ml.splitData(X,Y,0.8)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 91)\n"
     ]
    }
   ],
   "source": [
    "##########Initialization of the Ensemble:##########\n",
    "\n",
    "'''\n",
    "We will start with an Ensemble of size 25...\n",
    "''' \n",
    "\n",
    "# Ensemble Variables\n",
    "size = 25  # the amount of learners in the ensemble\n",
    "features = 55 # the number of features to select from when bagging\n",
    "\n",
    "# Create the ensemble\n",
    "ensemble = [None] * size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Kaggle data has 91 dimensions, one of the type of learners we chose was a support vector machine, as they perform well in high dimensionality. First, we tested different types of kernels to find which one performed the best. It is obvious that linear kernel was not going to estimate the data correctly, so a linear kernal was not tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with rbf as kernal\n",
      "\tMSE of training data: 0.690871165546\n",
      "\tMSE of test data: 0.718522580681\n",
      "SVM with sigmoid as kernal\n",
      "\tMSE of training data: 0.780008793301\n",
      "\tMSE of test data: 0.814328865355\n",
      "SVM with poly as kernal\n",
      "\tMSE of training data: 8.00810410361e+26\n",
      "\tMSE of test data: 8.12192003453e+26\n"
     ]
    }
   ],
   "source": [
    "##########Code for the Support Vector Machines:##########\n",
    "\n",
    "# First we scale the data for the SVM\n",
    "XiTe = preprocessing.scale(Xte)\n",
    "XiTr = preprocessing.scale(Xtr)\n",
    "\n",
    "# Test accuracy of each type of SVM\n",
    "kernels = ['rbf','sigmoid','poly']\n",
    "\n",
    "for k in kernels:\n",
    "    clf = svm.SVR(kernel=k)\n",
    "    clf.fit(XiTr[:1000,],Ytr[:1000])\n",
    "    \n",
    "    YhatTrain = clf.predict(Xtr)\n",
    "    YhatTest = clf.predict(Xte)\n",
    "\n",
    "    MSEtrain = np.mean((Ytr - YhatTrain)**2)\n",
    "    MSEtest = np.mean((Yte - YhatTest)**2)\n",
    "\n",
    "    print(\"SVM with {} as kernal\".format(k))\n",
    "    print(\"\\tMSE of training data: \" + str(MSEtrain))\n",
    "    print(\"\\tMSE of test data: \" + str(MSEtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Based on the data, the RBF kernel had the best performance. Now, since SVM can take a long time with large amounts of data, we will see how long an SVM with a RBF kernel takes with different subsets of data and it's performance on said data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 5000 data points\n",
      "\tTraining: 3.24 seconds\n",
      "\tPredicting: 1.79 seconds\n",
      "\tMSE of data: 0.71\n",
      "Data for 10000 data points\n",
      "\tTraining: 12.60 seconds\n",
      "\tPredicting: 6.96 seconds\n",
      "\tMSE of data: 0.70\n",
      "Data for 20000 data points\n",
      "\tTraining: 180.33 seconds\n",
      "\tPredicting: 28.04 seconds\n",
      "\tMSE of data: 0.70\n",
      "Data for 40000 data points\n",
      "\tTraining: 890.83 seconds\n",
      "\tPredicting: 112.21 seconds\n",
      "\tMSE of data: 0.69\n",
      "Data for 60000 data points\n",
      "\tTraining: 3303.78 seconds\n",
      "\tPredicting: 245.58 seconds\n",
      "\tMSE of data: 0.70\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "clf = svm.SVR(kernel='rbf')\n",
    "Xi = preprocessing.scale(X)\n",
    "\n",
    "for i in [5000,10000,20000,40000,60000]:\n",
    "    print(\"Data for {} data points\".format(i))\n",
    "    t0 = time.time()\n",
    "    clf.fit(Xi[:i,],Y[:i])\n",
    "    \n",
    "    print(\"\\tTraining: {:.2f} seconds\".format(time.time()-t0))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    Yhat = clf.predict(X[:i,])\n",
    "    print(\"\\tPredicting: {:.2f} seconds\".format(time.time()-t0))\n",
    "    \n",
    "    MSE = np.mean((Y[:i] - Yhat)**2)\n",
    "\n",
    "    print(\"\\tMSE of data: {:.2f}\".format(MSE))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Based on these results, it is best to train on a subset of the data of size 20000 since the accuracy is not improved much after 20000 data points but is taking much longer. This number will be used when training the final data in the ensemble.  \n",
    "  \n",
    "Now, all that is left to do is create an ensemble. Since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = [None] * 8\n",
    "\n",
    "# Train model\n",
    "for i in range(8):\n",
    "    Xi,Yi = ml.utils.bootstrapData(Xtr,Ytr)\n",
    "    temp[i] = svm.SVR(kernel='rbf')\n",
    "    temp[i].fit(XiTr,Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now predict\n",
    "mTest = Xte.shape[0]\n",
    "predict = np.zeros((mTest,8))\n",
    "for learner in temp:\n",
    "    predict[:,i] = temp[i].predict(Xte)\n",
    "    \n",
    "predict = np.mean(predict,axis=1)\n",
    "\n",
    "print(\"MSE of ensemble of length 8: {}\".format(np.mean((Yte-predict)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########Code for the KNN Learner:##########\n",
    "from sklearn.neighbors import KNeighborsRegressor  #Imported Library for KNN Regressor \n",
    "\n",
    "'''\n",
    "KNN Learner INFO... \n",
    "-describe problem you use chose and methods to address it\n",
    "-how did you train the models?\n",
    "-how you selected any parameters each model/method requires \n",
    "-how they performed on test data\n",
    "-consider table of performance of different approaches or plots of perofrmance used to perform model selection\n",
    "''' \n",
    "\n",
    "'''\n",
    "Key Points:\n",
    "-Explore some aspect of prediction that we have not already done in depth \n",
    "-Identify a paper that proposes a method you think could be helpful \n",
    "-Use stacking/information from your leaderboard performance to try and improve your prediction quality \n",
    "-To explore approach: explore method from class fully enough to understand how changes might affect its performance,\n",
    "verify your findings make sense, and then use your findings to optimize performance\n",
    "-In Your Report: describe why you decided to explore this aspect, what you expected to find, and how your findings\n",
    "matched/ didnt match your expectations \n",
    "-Beware of the positive/negative aspects of the learners we discussed ie Nearest Neighbor methods can be powerful but can\n",
    "also be slow for large data sets...perhaps you can reduce the data in some way without sacrificing performance (bootstrap\n",
    "aggregation)\n",
    "-Linear methods may be fast but do not provide enough model complexity to provide a good fit so you may been to try and \n",
    "generate better features \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Code for the Decision Tree:##########\n",
    "#Imported Library for Decision Trees   \n",
    "\n",
    "'''\n",
    "Decision Tree Learner INFO... \n",
    "'''\n",
    "\n",
    "# Decision Tree Variables\n",
    "depth = 20 # the maxth depth of the decision tree\n",
    "nodes = 8 # the minimum number of data to split node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Code to store each learner in the ensemble:##########\n",
    "\n",
    "'''\n",
    "How should we store the learners we come up with? Since right now we have an ensemble of size 25 we could maybe \n",
    "create a list for each of our learners and bring them here then index each list of learners, train it and then store \n",
    "it into the ensemble before making all the predictions. What do you guys think?  \n",
    "'''\n",
    "# Create learners and add to ensemble\n",
    "\n",
    "for i in range(size):\n",
    "    \n",
    "    #for learners in each list \n",
    "    \n",
    "        #get KNN learner \n",
    "        #train KNN learner \n",
    "        #ensemble[i] = KNN Learner \n",
    "        \n",
    "        #get NN learner \n",
    "        #train NN learner \n",
    "        #ensemble[i] = NN Learner \n",
    "\n",
    "        #get DT learner \n",
    "        #train DT learner \n",
    "        #ensemble[i] = DT Learner \n",
    "        \n",
    "    #dt = dtree.treeRegress()\n",
    "    #Xi,Yi = ml.utils.bootstrapData(Xtr,Ytr)\n",
    "    #dt.train(Xi,Yi,maxdepth=depth,nFeatures=features,minParent=nodes)\n",
    "    \n",
    "    #ensemble[i] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Code to output the predictions and evaluate them on kaggle:##########\n",
    "\n",
    "# Test correctness of ensemble through MSE\n",
    "mTest = Xte.shape[0] # Acquire the shape of the test data\n",
    "Yhat = np.zeros((mTest,num))\n",
    "MSE = 0\n",
    "\n",
    "for i in range(size):\n",
    "    Yhat[:,i] = ensemble[i].predict(Xte).reshape(mTest)\n",
    "    \n",
    "    Yhat = np.mean(Yhat,axis=1)\n",
    "    \n",
    "    MSE = np.mean((Yte - Yhat.reshape(Yte.shape))**2,axis=0)\n",
    "    \n",
    "print(MSE)\n",
    "\n",
    "'''\n",
    "Note:\n",
    "-Should not try to upload every possible model with every possible parameter setting \n",
    "-Use validation data, or cross-validation to assess which models are worth uploading, and just use the uploads\n",
    "to verify performance. \n",
    "'''\n",
    "#Ye = learner.predict( Xeval ); # make predictions\n",
    "# Note: be sure Ye is a flat vector, shape (m,)\n",
    "# otherwise, reshape it using e.g.\n",
    "# Ye = Ye.ravel()\n",
    "# or change the indexing in the code below:\n",
    "fh = open('predictions.csv','w') # open file for upload\n",
    "fh.write('ID,Prediction\\n') # output header line\n",
    "for i,yi in enumerate(Ye):\n",
    "fh.write('{},{}\\n'.format(i+1,yi)) # output each prediction\n",
    "fh.close() # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<i><h2 align='center'>Conclusion</h2></i>  \n",
    "Here we can probably summarize our results and the learners we were responsible for.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
